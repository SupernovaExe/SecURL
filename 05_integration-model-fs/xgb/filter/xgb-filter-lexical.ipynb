{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Classifier with Filter-Based FS (Lexical + Content)\n",
    "\n",
    "Steven Sison | March 9, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document will be used to train a model using the reduced feature set obtain by using the wrapper-based method, forward feature selection. The model will be evaluated in terms of the usual metrics (accuracy, precision, F1-score, recall) as well as the training time. The model will also be stored for future evaluation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                     # For data transformation\n",
    "import numpy as numpy                   # For scientific calculations\n",
    "import seaborn as sns                   # For data visualizations\n",
    "import matplotlib.pyplot as plt         # For plotting\n",
    "import plotly.graph_objects as go       # For plotting\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, ConfusionMatrixDisplay\n",
    "from xgboost import XGBClassifier, DMatrix, train\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error # or any other metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv(\"../../../02_feature-engineering/final-datasets/binary_new_Bacud_unbalanced_lexical.csv\")      # Loading the dataset\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dataset.drop(columns=['url_type']), dataset['url_type'], test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing (Balancing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['url_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Removing Unnecessary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features_lexical = ['url_host_length',\n",
    "                             'url_is_https',\n",
    "                             'url_ip_in_domain',\n",
    "                             'has_php_in_string',\n",
    "                             'url_number_of_parameters',\n",
    "                             'has_exe_in_string',\n",
    "                             'url_has_port',\n",
    "                             'url_is_digits_in_domain',\n",
    "                             'url_path_length',\n",
    "                             'url_num_question_mark', \n",
    "                             'url_query_length',\n",
    "                             'url_string_entropy',\n",
    "                             'url_num_periods',\n",
    "                             'get_tld',\n",
    "                             'url_scheme']\n",
    "\n",
    "X_test_lexical = x_test[important_features_lexical]\n",
    "X_train_lexical = x_train[important_features_lexical]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective_lexical(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    param = {\n",
    "        'objective': 'binary:hinge',\n",
    "        'eval_metric': 'error',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'n_estimators': 100000, # Fix the boosting round and use early stopping\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 10.0),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 10.0),\n",
    "        'lambda': trial.suggest_float('lambda', 0.1, 10.0),\n",
    "        'alpha': trial.suggest_float('alpha', 0.0, 10.0),\n",
    "    }\n",
    "    \n",
    "    # Split the data into further training and validation sets (three sets are preferable)\n",
    "    train_data, valid_data, train_target, valid_target = train_test_split(X_train_lexical, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Convert the data into DMatrix format\n",
    "    dtrain = DMatrix(train_data, label=train_target)\n",
    "    dvalid = DMatrix(valid_data, label=valid_target)\n",
    "    \n",
    "    # Define the pruning callback for early stopping\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, 'validation-error')\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    model = train(param, dtrain, num_boost_round=100000, evals=[(dvalid, 'validation')], early_stopping_rounds=100, callbacks=[pruning_callback])\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    dtest = DMatrix(valid_data)\n",
    "    y_pred = model.predict(dtest)\n",
    "    \n",
    "    # Calculate the root mean squared error\n",
    "    error = mean_squared_error(valid_target, y_pred, squared=False)\n",
    "    \n",
    "    return error\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study_lexical = optuna.create_study(direction='minimize')\n",
    "study_lexical.optimize(objective_lexical, n_trials=100) # Control the number of trials\n",
    "\n",
    "# Print the best hyperparameters and the best RMSE\n",
    "best_params_lexical = study_lexical.best_params\n",
    "best_error_lexical = study_lexical.best_value\n",
    "print(\"Best Hyperparameters (33 Features): \", best_params_lexical)\n",
    "print(\"Best Error (33 Features): \", best_error_lexical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from xgboost import DMatrix, train\n",
    "\n",
    "# Initialize CV\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "best_params_lexical['objective'] = 'binary:hinge'\n",
    "best_params_lexical['eval_metric'] = 'error'\n",
    "\n",
    "# Convert the data into DMatrix format\n",
    "lexical_train = DMatrix(X_train_lexical, label=y_train)\n",
    "lexical_valid = DMatrix(X_test_lexical, label=y_test)\n",
    "\n",
    "# Train the Model\n",
    "xgb_classifier_lexical = train(best_params_lexical, lexical_train, num_boost_round=3000)\n",
    "y_pred_lexical = xgb_classifier_lexical.predict(lexical_valid)\n",
    "\n",
    "print(\"Model training done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred_lexical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping the model\n",
    "joblib.dump(xgb_classifier_lexical, 'xgb_filter_lexical.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lexical_generator_filter_lexical\n",
    "import time\n",
    "\n",
    "def xgb_predict_maliciousness(url):\n",
    "\n",
    "    numerical_values = lexical_generator_filter_lexical.lexical_generator(url)\n",
    "    # print(numerical_values)\n",
    "    numerical_values = DMatrix(numerical_values)\n",
    "\n",
    "    match xgb_classifier_lexical.predict(numerical_values):\n",
    "        case 0:\n",
    "            return \"Benign\"\n",
    "        case 1:\n",
    "            return \"Malware\"\n",
    "        case 2:\n",
    "            return \"Phishing\"\n",
    "        case 3:\n",
    "            return \"Defacement\"\n",
    "\n",
    "url = \"www.facebook.com/\"\n",
    "print(\"Current URL: \"+url)\n",
    "\n",
    "print(\"------------- Filter-Based (Lexical) -------------\")\n",
    "for i in range(15):\n",
    "    start = time.perf_counter()\n",
    "    prediction = xgb_predict_maliciousness(url)\n",
    "    end = time.perf_counter()\n",
    "    print(\"Trial \"+str(i))\n",
    "    print(prediction)\n",
    "    print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Confusion Matrix for 12 Features\n",
    "cm_up = confusion_matrix(y_test, y_pred, labels=xgb_classifier.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm_up, display_labels = xgb_classifier.classes_)\n",
    "disp.plot()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Cross Validation Score\n",
    "scores = cross_val_score(XGBClassifier(**params_gbm),\n",
    "                        X_train, y_train, scoring='accuracy', cv=cv).mean()\n",
    "\n",
    "print(scores)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
