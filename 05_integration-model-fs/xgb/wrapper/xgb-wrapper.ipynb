{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Classifier with Wrapper-Based FS\n",
    "\n",
    "Steven Sison | March 9, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document will be used to train a model using the reduced feature set obtain by using the wrapper-based method, forward feature selection. The model will be evaluated in terms of the usual metrics (accuracy, precision, F1-score, recall) as well as the training time. The model will also be stored for future evaluation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                     # For data transformation\n",
    "import numpy as numpy                   # For scientific calculations\n",
    "import seaborn as sns                   # For data visualizations\n",
    "import matplotlib.pyplot as plt         # For plotting\n",
    "import plotly.graph_objects as go       # For plotting\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, ConfusionMatrixDisplay\n",
    "from xgboost import XGBClassifier, DMatrix, train\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "dataset = pd.read_csv(\"../../../02_feature-engineering/final-datasets/binary_new_Bacud_unbalanced_lexical.csv\")      # Loading the dataset\n",
    "\n",
    "dataset.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dataset.drop(columns=['url_type']), dataset['url_type'], test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing (Balancing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['url_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Removing Unnecessary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features_wrapper_33 = ['url_length',\n",
    " 'url_domain_entropy',\n",
    " 'url_is_digits_in_domain',\n",
    " 'url_number_of_parameters',\n",
    " 'url_number_of_digits',\n",
    " 'url_string_entropy',\n",
    " 'url_path_length',\n",
    " 'url_host_length',\n",
    " 'get_tld',\n",
    " 'url_domain_len',\n",
    " 'url_num_subdomain',\n",
    " 'url_number_of_fragments',\n",
    " 'url_is_encoded',\n",
    " 'url_number_of_letters',\n",
    " 'url_num_periods',\n",
    " 'url_num_of_hyphens',\n",
    " 'url_num_underscore',\n",
    " 'url_num_forward_slash',\n",
    " 'url_num_semicolon',\n",
    " 'url_num_mod_sign',\n",
    " 'has_login_in_string',\n",
    " 'has_signin_in_string',\n",
    " 'has_logon_in_string',\n",
    " 'has_loginasp_in_string',\n",
    " 'has_exe_in_string',\n",
    " 'has_viewerphp_in_string',\n",
    " 'has_getImageasp_in_string',\n",
    " 'has_paypal_in_string',\n",
    " 'has_dbsysphp_in_string',\n",
    " 'has_shopping_in_string',\n",
    " 'has_php_in_string',\n",
    " 'has_bin_in_string',\n",
    " 'has_personal_in_string',\n",
    " 'url_scheme'\n",
    " ]\n",
    "\n",
    "important_features_wrapper_12 = ['url_domain_entropy', \n",
    "                              'url_number_of_parameters', \n",
    "                              'url_number_of_digits', \n",
    "                              'url_path_length', \n",
    "                              'url_host_length', \n",
    "                              'get_tld', \n",
    "                              'url_domain_len', \n",
    "                              'url_num_subdomain', \n",
    "                              'url_number_of_letters', \n",
    "                              'url_num_periods', \n",
    "                              'url_num_of_hyphens', \n",
    "                              'url_num_forward_slash', \n",
    "                              'url_num_semicolon', \n",
    "                              'has_login_in_string', \n",
    "                              'has_exe_in_string', \n",
    "                              'has_php_in_string', \n",
    "                              'url_scheme']\n",
    "\n",
    "X_test_12 = x_test[important_features_wrapper_12]\n",
    "X_train_12 = x_train[important_features_wrapper_12]\n",
    "\n",
    "X_test_33 = x_test[important_features_wrapper_33]\n",
    "X_train_33 = x_train[important_features_wrapper_33]\n",
    "\n",
    "X_test_33.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12 Features (Purely Lexical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error # or any other metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective_12(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    param = {\n",
    "        'objective': 'binary:hinge',\n",
    "        'eval_metric': 'error',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'n_estimators': 100000, # Fix the boosting round and use early stopping\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 10.0),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 10.0),\n",
    "        'lambda': trial.suggest_float('lambda', 0.1, 10.0),\n",
    "        'alpha': trial.suggest_float('alpha', 0.0, 10.0),\n",
    "    }\n",
    "    \n",
    "    # Split the data into further training and validation sets (three sets are preferable)\n",
    "    train_data, valid_data, train_target, valid_target = train_test_split(X_train_12, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Convert the data into DMatrix format\n",
    "    dtrain = DMatrix(train_data, label=train_target)\n",
    "    dvalid = DMatrix(valid_data, label=valid_target)\n",
    "    \n",
    "    # Define the pruning callback for early stopping\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, 'validation-error')\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    model = train(param, dtrain, num_boost_round=100000, evals=[(dvalid, 'validation')], early_stopping_rounds=100, callbacks=[pruning_callback])\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    dtest = DMatrix(valid_data)\n",
    "    y_pred = model.predict(dtest)\n",
    "    \n",
    "    # Calculate the root mean squared error\n",
    "    error = mean_squared_error(valid_target, y_pred, squared=False)\n",
    "    \n",
    "    return error\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study_12 = optuna.create_study(direction='minimize')\n",
    "study_12.optimize(objective_12, n_trials=100) # Control the number of trials\n",
    "\n",
    "# Print the best hyperparameters and the best RMSE\n",
    "best_params_12 = study_12.best_params\n",
    "best_error = study_12.best_value\n",
    "print(\"Best Hyperparameters (12 Features): \", best_params_12)\n",
    "print(\"Best Error (12 Features): \", best_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 33 Features (Purely Lexical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective_33(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    param = {\n",
    "        'objective': 'binary:hinge',\n",
    "        'eval_metric': 'error',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'n_estimators': 100000, # Fix the boosting round and use early stopping\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 10.0),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 10.0),\n",
    "        'lambda': trial.suggest_float('lambda', 0.1, 10.0),\n",
    "        'alpha': trial.suggest_float('alpha', 0.0, 10.0),\n",
    "    }\n",
    "    \n",
    "    # Split the data into further training and validation sets (three sets are preferable)\n",
    "    train_data, valid_data, train_target, valid_target = train_test_split(X_train_33, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Convert the data into DMatrix format\n",
    "    dtrain = DMatrix(train_data, label=train_target)\n",
    "    dvalid = DMatrix(valid_data, label=valid_target)\n",
    "    \n",
    "    # Define the pruning callback for early stopping\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, 'validation-error')\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    model = train(param, dtrain, num_boost_round=100000, evals=[(dvalid, 'validation')], early_stopping_rounds=100, callbacks=[pruning_callback])\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    dtest = DMatrix(valid_data)\n",
    "    y_pred = model.predict(dtest)\n",
    "    \n",
    "    # Calculate the root mean squared error\n",
    "    error = mean_squared_error(valid_target, y_pred, squared=False)\n",
    "    \n",
    "    return error\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study_33 = optuna.create_study(direction='minimize')\n",
    "study_33.optimize(objective_33, n_trials=100) # Control the number of trials\n",
    "\n",
    "# Print the best hyperparameters and the best RMSE\n",
    "best_params_33 = study_33.best_params\n",
    "best_error_33 = study_33.best_value\n",
    "print(\"Best Hyperparameters (33 Features): \", best_params_33)\n",
    "print(\"Best Error (33 Features): \", best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params_12)\n",
    "print(best_params_33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import optuna\n",
    "\n",
    "# Initialize CV\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# Set Hyperparameters\n",
    "params_12 = { 'objective': 'binary:hinge',\n",
    "              'eta': 0.2769198716475172, \n",
    "              'max_depth': 10, \n",
    "              'subsample': 0.82358642201105, \n",
    "              'colsample_bytree': 0.8286208231929323, \n",
    "              'gamma': 0.7766612702667438, \n",
    "              'min_child_weight': 8.319434489010376, \n",
    "              'lambda': 4.43143014244566, \n",
    "              'alpha': 4.399273966701367,\n",
    "              'eval_metric': 'error'\n",
    "              }\n",
    "\n",
    "best_params_12['objective'] = 'binary:hinge'\n",
    "best_params_12['eval_metric'] = 'error'\n",
    "\n",
    "best_params_33['objective'] = 'binary:hinge'\n",
    "best_params_33['eval_metric'] = 'error'\n",
    "\n",
    "# Convert the data into DMatrix format\n",
    "dtrain_12 = DMatrix(X_train_12, label=y_train)\n",
    "dvalid_12 = DMatrix(X_test_12, label=y_test)\n",
    "\n",
    "dtrain_33 = DMatrix(X_train_33, label=y_train)\n",
    "dvalid_33 = DMatrix(X_test_33, label=y_test)\n",
    "\n",
    "# Train the Model\n",
    "'''xgb_classifier_12 = train(best_params_12, dtrain_12, num_boost_round=3000)\n",
    "y_pred_12 = xgb_classifier_12.predict(dvalid_12)\n",
    "\n",
    "print(\"Model with 12 Features Done.\")'''\n",
    "\n",
    "xgb_classifier_33 = train(best_params_33, dtrain_33, num_boost_round=3000)\n",
    "y_pred_33 = xgb_classifier_33.predict(dvalid_33)\n",
    "\n",
    "print(\"Model with 33 Features Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred_12))\n",
    "print(classification_report(y_test, y_pred_33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Confusion Matrix for 12 Features\n",
    "cm_up = confusion_matrix(y_test, y_pred_12, labels=xgb_classifier_12.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm_up, display_labels = xgb_classifier_12.classes_)\n",
    "disp.plot()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Cross Validation Score\n",
    "scores = cross_val_score(XGBClassifier(**params_gbm),\n",
    "                        X_train, y_train, scoring='accuracy', cv=cv).mean()\n",
    "\n",
    "print(scores)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping the model\n",
    "joblib.dump(xgb_classifier_12, 'xgb_ffs_12.sav')\n",
    "joblib.dump(xgb_classifier_33, 'xgb_ffs_33.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lexical_generator_12\n",
    "import lexical_generator_33\n",
    "import time\n",
    "\n",
    "def xgb_predict_maliciousness_12(url):\n",
    "\n",
    "    numerical_values = lexical_generator_12.lexical_generator(url)\n",
    "    # print(numerical_values)\n",
    "    numerical_values = DMatrix(numerical_values)\n",
    "\n",
    "    match xgb_classifier_12.predict(numerical_values):\n",
    "        case 0:\n",
    "            return \"Benign\"\n",
    "        case 1:\n",
    "            return \"Malware\"\n",
    "        case 2:\n",
    "            return \"Phishing\"\n",
    "        case 3:\n",
    "            return \"Defacement\"\n",
    "        \n",
    "def xgb_predict_maliciousness_33(url):\n",
    "\n",
    "    numerical_values = lexical_generator_33.lexical_generator(url)\n",
    "    # print(numerical_values)\n",
    "    numerical_values = DMatrix(numerical_values)\n",
    "\n",
    "    match xgb_classifier_33.predict(numerical_values):\n",
    "        case 0:\n",
    "            return \"Benign\"\n",
    "        case 1:\n",
    "            return \"Malware\"\n",
    "        case 2:\n",
    "            return \"Phishing\"\n",
    "        case 3:\n",
    "            return \"Defacement\"\n",
    "\n",
    "url = \"youtube.com/watch?v=LF4Q4bR9SPw\"\n",
    "print(\"Current URL: \"+url)\n",
    "\n",
    "start = time.perf_counter()\n",
    "prediction = xgb_predict_maliciousness_12(url)\n",
    "end = time.perf_counter()\n",
    "print(\"------- 12 Features -------------\")\n",
    "print(prediction)\n",
    "print(end-start)\n",
    "\n",
    "'''start = time.perf_counter()\n",
    "prediction = xgb_predict_maliciousness_33(url)\n",
    "end = time.perf_counter()\n",
    "print(\"------- 33 Features -------------\")\n",
    "print(prediction)\n",
    "print(end-start)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
