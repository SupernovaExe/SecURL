{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF Classifier with Wrapper-Based FS\n",
    "\n",
    "Steven Sison | March 9, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document will be used to train a model using the reduced feature set obtain by using the wrapper-based method, forward feature selection. The model will be evaluated in terms of the usual metrics (accuracy, precision, F1-score, recall) as well as the training time. The model will also be stored for future evaluation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                     # For data transformation\n",
    "import numpy as numpy                   # For scientific calculations\n",
    "import seaborn as sns                   # For data visualizations\n",
    "import matplotlib.pyplot as plt         # For plotting\n",
    "import plotly.graph_objects as go       # For plotting\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "dataset = pd.read_csv(\"../../../02_feature-engineering/final-datasets/binary_new_Bacud_unbalanced_lexical.csv\")      # Loading the dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset.drop(columns=['url_type']), dataset['url_type'], test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing (Balancing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['url_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Removing Unnecessary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features_wrapper_19 = ['url_domain_entropy', \n",
    "                                 'url_is_digits_in_domain', \n",
    "                                 'url_number_of_digits', \n",
    "                                 'url_is_https', \n",
    "                                 'url_path_length', \n",
    "                                 'url_host_length', \n",
    "                                 'get_tld', \n",
    "                                 'url_domain_len', \n",
    "                                 'url_num_subdomain', \n",
    "                                 'url_num_periods', \n",
    "                                 'url_num_of_hyphens', \n",
    "                                 'url_num_underscore', \n",
    "                                 'url_num_equal', \n",
    "                                 'url_num_forward_slash', \n",
    "                                 'has_login_in_string', \n",
    "                                 'has_exe_in_string', \n",
    "                                 'has_linkeq_in_string', \n",
    "                                 'has_paypal_in_string', \n",
    "                                 'has_php_in_string']\n",
    "\n",
    "important_features_wrapper_30 = ['url_domain_entropy',\n",
    "                                 'url_is_digits_in_domain', \n",
    "                                 'url_number_of_digits', \n",
    "                                 'url_is_https', \n",
    "                                 'url_path_length', \n",
    "                                 'url_host_length', \n",
    "                                 'get_tld', \n",
    "                                 'url_domain_len', \n",
    "                                 'url_num_subdomain', \n",
    "                                 'url_is_encoded', \n",
    "                                 'url_num_periods', \n",
    "                                 'url_num_of_hyphens', \n",
    "                                 'url_num_underscore', \n",
    "                                 'url_num_equal', \n",
    "                                 'url_num_forward_slash', \n",
    "                                 'url_num_question_mark', \n",
    "                                 'url_num_semicolon', \n",
    "                                 'url_num_at', \n",
    "                                 'has_secure_in_string', \n",
    "                                 'has_login_in_string', \n",
    "                                 'has_ebayisapi_in_string', \n",
    "                                 'has_exe_in_string', \n",
    "                                 'has_jpg_in_string', \n",
    "                                 'has_linkeq_in_string', \n",
    "                                 'has_paypal_in_string', \n",
    "                                 'has_mailphp_in_string', \n",
    "                                 'has_php_in_string', \n",
    "                                 'has_admin_in_string', \n",
    "                                 'has_personal_in_string', \n",
    "                                 'has_update_in_string']\n",
    "\n",
    "X_test_19 = x_test[important_features_wrapper_19]\n",
    "X_train_19 = x_train[important_features_wrapper_19]\n",
    "\n",
    "X_test_30 = x_test[important_features_wrapper_30]\n",
    "X_train_30 = x_train[important_features_wrapper_30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19 Features (Purely Lexical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error # or any other metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective_19(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 100),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 2, 100),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 100),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 300, 3000),\n",
    "        'max_samples': trial.suggest_float('max_sample', 0, 1),\n",
    "        'max_features': trial.suggest_int('max_features', 1, len(important_features_wrapper_19))\n",
    "    }\n",
    "    \n",
    "    # Split the data into further training and validation sets (three sets are preferable)\n",
    "    train_data, valid_data, train_target, valid_target = train_test_split(X_train_19, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    scores = cross_val_score(RandomForestClassifier(random_state=123, **param, warm_start = True, n_jobs = 16),\n",
    "                             X_train_19, y_train, scoring='neg_mean_absolute_error', cv=5).mean()\n",
    "    score = scores.mean()\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study_19 = optuna.create_study(direction='maximize')\n",
    "study_19.optimize(objective_19, n_trials=25) # Control the number of trials\n",
    "\n",
    "# Print the best hyperparameters and the best RMSE\n",
    "best_params_19 = study_19.best_params\n",
    "best_error_19 = study_19.best_value\n",
    "print(\"Best Hyperparameters (12 Features): \", best_params_19)\n",
    "print(\"Best Error (12 Features): \", best_error_19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30 Features (Purely Lexical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error # or any other metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective_30(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 100),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 2, 100),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 100),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 300, 3000),\n",
    "        'max_samples': trial.suggest_float('max_sample', 0, 1),\n",
    "        'max_features': trial.suggest_int('max_features', 1, len(important_features_wrapper_19))\n",
    "    }\n",
    "    \n",
    "    # Split the data into further training and validation sets (three sets are preferable)\n",
    "    train_data, valid_data, train_target, valid_target = train_test_split(X_train_30, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    scores = cross_val_score(RandomForestClassifier(random_state=123, **param, warm_start = True, n_jobs = 16),\n",
    "                             X_train_30, y_train, scoring='neg_mean_absolute_error', cv=5).mean()\n",
    "    score = scores.mean()\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study_30 = optuna.create_study(direction='maximize')\n",
    "study_30.optimize(objective_30, n_trials=25) # Control the number of trials\n",
    "\n",
    "# Print the best hyperparameters and the best RMSE\n",
    "best_params_30 = study_30.best_params\n",
    "best_error_30 = study_30.best_value\n",
    "print(\"Best Hyperparameters (12 Features): \", best_params_30)\n",
    "print(\"Best Error (12 Features): \", best_error_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Initialize CV\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize the model\n",
    "rf_classifier_19 = Pipeline([\n",
    "    ('classifier', RandomForestClassifier(random_state=123, **best_params_19, warm_start = True, n_jobs = 16))\n",
    "])\n",
    "\n",
    "rf_classifier_30 = Pipeline([\n",
    "    ('classifier', RandomForestClassifier(random_state=123, **best_params_30, warm_start = True, n_jobs = 16))\n",
    "])\n",
    "\n",
    "# Train the Model\n",
    "rf_classifier_19.fit(X_train_19, y_train)\n",
    "y_pred_19 = rf_classifier_19.predict(X_test_19)\n",
    "\n",
    "rf_classifier_30.fit(X_train_30, y_train)\n",
    "y_pred_30 = rf_classifier_30.predict(X_test_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred_19))\n",
    "print(classification_report(y_test, y_pred_30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Confusion Matrix for 12 Features\n",
    "cm_up = confusion_matrix(y_test, y_pred_10, labels=rf_classifier_19.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm_up, display_labels = rf_classifier_19.classes_)\n",
    "disp.plot()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Cross Validation Score\n",
    "scores = cross_val_score(XGBClassifier(random_state=45, **params_gbm),\n",
    "                        x_train, y_train, scoring='accuracy', cv=cv).mean()\n",
    "\n",
    "print(scores)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping the model\n",
    "joblib.dump(rf_classifier_19, 'rf_ffs_12.sav')\n",
    "joblib.dump(rf_classifier_30, 'rf_ffs_33.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lexical_generator_19'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlexical_generator_19\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlexical_generator_30\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lexical_generator_19'"
     ]
    }
   ],
   "source": [
    "import lexical_generator_19\n",
    "import lexical_generator_30\n",
    "import time\n",
    "\n",
    "def rf_predict_maliciousness_19(url):\n",
    "\n",
    "    numerical_values = lexical_generator_19.lexical_generator(url)\n",
    "\n",
    "    match rf_classifier_19.predict(numerical_values):\n",
    "        case 0:\n",
    "            return \"Benign\"\n",
    "        case 1:\n",
    "            return \"Defacement\"\n",
    "        case 2:\n",
    "            return \"Phishing\"\n",
    "        case 3:\n",
    "            return \"Malware\"\n",
    "        \n",
    "def rf_predict_maliciousness_30(url):\n",
    "\n",
    "    numerical_values = lexical_generator_30.lexical_generator(url)\n",
    "\n",
    "    numerical_values.head()\n",
    "\n",
    "    match rf_classifier_30.predict(numerical_values):\n",
    "        case 0:\n",
    "            return \"Benign\"\n",
    "        case 1:\n",
    "            return \"Defacement\"\n",
    "        case 2:\n",
    "            return \"Phishing\"\n",
    "        case 3:\n",
    "            return \"Malware\"\n",
    "\n",
    "url = \"www.youtube.com/watch?v=RJM5rF-aluM\"\n",
    "print(\"Current URL: \"+url)\n",
    "\n",
    "start = time.perf_counter()\n",
    "prediction = rf_predict_maliciousness_19(url)\n",
    "end = time.perf_counter()\n",
    "print(\"------- 19 Features -------------\")\n",
    "print(prediction)\n",
    "print(end-start)\n",
    "\n",
    "start = time.perf_counter()\n",
    "prediction = rf_predict_maliciousness_30(url)\n",
    "end = time.perf_counter()\n",
    "print(\"------- 30 Features -------------\")\n",
    "print(prediction)\n",
    "print(end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
