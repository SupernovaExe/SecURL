{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Gradient Boosting Classifier\n",
    "\n",
    "By Steven Sison on December 16, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This document will be used for the preliminary training and evaluation of the extreme gradient boosting classifier. The document includes the necessary processes taken to train the model with the default hyperparameters. This also evaluates the performance of the classifier in terms of accuracy, precision, recall, F1-score, training time, and detection time. Furthermore, this document will only use lexical features and will observe the effect of increasing the number of features used in the model. As this is only for preliminary work, no optimizations, except a simple train-test validation, will be carried out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                     # For data transformation\n",
    "import numpy as numpy                   # For scientific calculations\n",
    "import seaborn as sns                   # For data visualizations\n",
    "import matplotlib.pyplot as plt         # For plotting\n",
    "import plotly.graph_objects as go       # For plotting\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, ConfusionMatrixDisplay\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "dataset = pd.read_csv(\"final_unbalanced_withLexical.csv\")      # Loading the dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset.drop(columns=['url_type']), dataset['url_type'], test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model using All Lexical Features Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = XGBClassifier()\n",
    "\n",
    "pipeline.fit(x_train, y_train)\n",
    "\n",
    "# pipeline = XGBClassifier()\n",
    "pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = pipeline.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Effect of Balanced and Unbalanced Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['url_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "dataset_benign = dataset[(dataset['url_type'] == 0)]\n",
    "dataset_defacement = dataset[(dataset['url_type'] == 1)]\n",
    "dataset_phishing = dataset[(dataset['url_type'] == 2)]\n",
    "dataset_malware = dataset[(dataset['url_type'] == 3)]\n",
    "\n",
    "dataset_defacement_upsampled = resample(dataset_defacement,\n",
    "                                        replace=True,\n",
    "                                        n_samples = dataset_benign.shape[0],\n",
    "                                        random_state = 15)\n",
    "\n",
    "dataset_phishing_upsampled = resample(dataset_phishing,\n",
    "                                        replace=True,\n",
    "                                        n_samples = dataset_benign.shape[0],\n",
    "                                        random_state = 15)\n",
    "\n",
    "dataset_malware_upsampled = resample(dataset_malware,\n",
    "                                        replace=True,\n",
    "                                        n_samples = dataset_benign.shape[0],\n",
    "                                        random_state = 15)\n",
    "\n",
    "dataset_benign_upsampled = resample(dataset_benign,\n",
    "                                        replace=True,\n",
    "                                        n_samples = dataset_benign.shape[0],\n",
    "                                        random_state = 15)\n",
    "\n",
    "dataset_upsampled = pd.concat([dataset_benign_upsampled, dataset_defacement_upsampled, dataset_malware_upsampled, dataset_phishing_upsampled])\n",
    "\n",
    "# dataset_upsampled.info(0)\n",
    "dataset_upsampled['url_type'].value_counts()\n",
    "\n",
    "x_up_train, x_up_test, y_up_train, y_up_test = train_test_split(dataset_upsampled.drop(columns=['url_type']), dataset_upsampled['url_type'], test_size = 0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_up = Pipeline([\n",
    "    ('classifier', XGBClassifier())\n",
    "])\n",
    "\n",
    "pipeline_up.fit(x_up_train, y_up_train)\n",
    "y_up_pred = pipeline_up.predict(x_up_test)\n",
    "print(classification_report(y_up_test, y_up_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_up = confusion_matrix(y_up_test, y_up_pred, labels=pipeline_up.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm_up, display_labels = pipeline_up.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        XGBClassifier(), x_up_train.to_numpy(), y_up_train.to_numpy(), x_up_test.to_numpy(), y_up_test.to_numpy(), \n",
    "        loss='0-1_loss',\n",
    "        random_seed=42)\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Effect of Adding more Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(3):\n",
    "    pipeline = XGBClassifier()\n",
    "\n",
    "    temp_url_features = x_up_train.iloc[:, 0:(25*(i+1))]\n",
    "    \n",
    "    pipeline.fit(temp_url_features, y_up_train)\n",
    "    \n",
    "    pipeline.save_model('xgb_lexical_{}.json'.format((25*(i+1))))\n",
    "\n",
    "    url_type_predict = pipeline.predict(x_up_test.iloc[:, 0:(25*(i+1))])\n",
    "\n",
    "    accuracy = accuracy_score(y_up_test, url_type_predict)\n",
    "    recall = recall_score(y_up_test, url_type_predict, average = 'weighted')\n",
    "    precision = precision_score(y_up_test, url_type_predict, average = 'weighted', zero_division=1)\n",
    "    f1 = f1_score(y_up_test, url_type_predict, average = 'weighted')\n",
    "    results.append(((4*(i+1)), accuracy, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results, columns=['Number of Features', 'Accuracy', 'Recall', 'Precision', 'F1-Score'])\n",
    "results = results.sort_values(by='Number of Features', ascending=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Increasing the number of features improves all the class weighted metrics of the model at the cost of a higher training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Test for K (No Hyperparameter Tuning but using Balanced Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def evaluating_model(cv):\n",
    "\n",
    "    X, y = dataset_upsampled.drop(columns=['url_type']), dataset_upsampled['url_type']\n",
    "\n",
    "    model = XGBClassifier()\n",
    "    scores = cross_val_score(model, X,y, scoring = \"accuracy\", cv = cv, n_jobs=1)\n",
    "\n",
    "    return mean(scores), scores.min(), scores.max()\n",
    "\n",
    "# Getting the Ideal Score\n",
    "'''ideal, _, _ = evaluating_model(LeaveOneOut())\n",
    "print('Ideal: %.3f' % ideal)'''\n",
    "\n",
    "folds = range(10,11)\n",
    "\n",
    "means, mins, maxs = list(), list(), list()\n",
    "\n",
    "for k in folds:\n",
    "    # define the test condition\n",
    "    cv = KFold(n_splits=k, shuffle=True, random_state=1)\n",
    "    # evaluate k value\n",
    "    k_mean, k_min, k_max = evaluating_model(cv)\n",
    "    # report performance\n",
    "    print('> folds=%d, accuracy=%.3f (%.3f,%.3f)' % (k, k_mean, k_min, k_max))\n",
    "    # store mean accuracy\n",
    "    means.append(k_mean)\n",
    "    # store min and max relative to the mean\n",
    "    mins.append(k_mean - k_min)\n",
    "    maxs.append(k_max - k_mean)\n",
    "\n",
    "# line plot of k mean values with min/max error bars\n",
    "pyplot.errorbar(folds, means, yerr=[mins, maxs], fmt='o')\n",
    "# plot the ideal case in a separate color\n",
    "# pyplot.plot(folds, [ideal for _ in range(len(folds))], color='r')\n",
    "# show the plot\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization (Grid Search) and Cross Validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | colsam... | learni... | max_depth | n_esti... | reg_alpha | reg_la... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.9499   \u001b[0m | \u001b[0m0.6122   \u001b[0m | \u001b[0m0.1691   \u001b[0m | \u001b[0m0.4417   \u001b[0m | \u001b[0m8.385    \u001b[0m | \u001b[0m100.7    \u001b[0m | \u001b[0m0.7458   \u001b[0m | \u001b[0m0.1124   \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.954    \u001b[0m | \u001b[95m0.4202   \u001b[0m | \u001b[95m0.2387   \u001b[0m | \u001b[95m0.3443   \u001b[0m | \u001b[95m9.935    \u001b[0m | \u001b[95m96.64    \u001b[0m | \u001b[95m0.406    \u001b[0m | \u001b[95m3.348    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.9261   \u001b[0m | \u001b[0m0.6212   \u001b[0m | \u001b[0m0.2743   \u001b[0m | \u001b[0m0.4716   \u001b[0m | \u001b[0m3.829    \u001b[0m | \u001b[0m85.18    \u001b[0m | \u001b[0m4.504    \u001b[0m | \u001b[0m3.97     \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m0.9674   \u001b[0m | \u001b[95m0.8406   \u001b[0m | \u001b[95m0.8152   \u001b[0m | \u001b[95m0.991    \u001b[0m | \u001b[95m7.041    \u001b[0m | \u001b[95m137.0    \u001b[0m | \u001b[95m2.107    \u001b[0m | \u001b[95m0.1372   \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.9431   \u001b[0m | \u001b[0m0.4541   \u001b[0m | \u001b[0m0.1053   \u001b[0m | \u001b[0m0.819    \u001b[0m | \u001b[0m7.884    \u001b[0m | \u001b[0m119.6    \u001b[0m | \u001b[0m1.371    \u001b[0m | \u001b[0m4.992    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.9448   \u001b[0m | \u001b[0m0.138    \u001b[0m | \u001b[0m0.6154   \u001b[0m | \u001b[0m0.4902   \u001b[0m | \u001b[0m5.835    \u001b[0m | \u001b[0m130.9    \u001b[0m | \u001b[0m1.614    \u001b[0m | \u001b[0m2.003    \u001b[0m |\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m0.9685   \u001b[0m | \u001b[95m0.319    \u001b[0m | \u001b[95m0.9472   \u001b[0m | \u001b[95m0.9195   \u001b[0m | \u001b[95m8.697    \u001b[0m | \u001b[95m82.39    \u001b[0m | \u001b[95m4.717    \u001b[0m | \u001b[95m4.752    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.9597   \u001b[0m | \u001b[0m0.8066   \u001b[0m | \u001b[0m0.4813   \u001b[0m | \u001b[0m0.9671   \u001b[0m | \u001b[0m5.919    \u001b[0m | \u001b[0m102.3    \u001b[0m | \u001b[0m0.08221  \u001b[0m | \u001b[0m0.1909   \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.7952   \u001b[0m | \u001b[0m0.05309  \u001b[0m | \u001b[0m0.128    \u001b[0m | \u001b[0m0.04305  \u001b[0m | \u001b[0m4.585    \u001b[0m | \u001b[0m118.1    \u001b[0m | \u001b[0m0.8931   \u001b[0m | \u001b[0m0.8967   \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.9545   \u001b[0m | \u001b[0m0.1496   \u001b[0m | \u001b[0m0.683    \u001b[0m | \u001b[0m0.4743   \u001b[0m | \u001b[0m8.378    \u001b[0m | \u001b[0m126.8    \u001b[0m | \u001b[0m2.206    \u001b[0m | \u001b[0m2.976    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.9504   \u001b[0m | \u001b[0m0.4617   \u001b[0m | \u001b[0m0.9633   \u001b[0m | \u001b[0m0.1563   \u001b[0m | \u001b[0m9.348    \u001b[0m | \u001b[0m114.9    \u001b[0m | \u001b[0m2.962    \u001b[0m | \u001b[0m3.423    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.9189   \u001b[0m | \u001b[0m0.1775   \u001b[0m | \u001b[0m0.8647   \u001b[0m | \u001b[0m0.4873   \u001b[0m | \u001b[0m3.485    \u001b[0m | \u001b[0m105.5    \u001b[0m | \u001b[0m1.808    \u001b[0m | \u001b[0m2.36     \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.9321   \u001b[0m | \u001b[0m0.1795   \u001b[0m | \u001b[0m0.1753   \u001b[0m | \u001b[0m0.9496   \u001b[0m | \u001b[0m7.59     \u001b[0m | \u001b[0m89.45    \u001b[0m | \u001b[0m4.639    \u001b[0m | \u001b[0m4.209    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.9044   \u001b[0m | \u001b[0m0.05278  \u001b[0m | \u001b[0m0.2037   \u001b[0m | \u001b[0m0.8738   \u001b[0m | \u001b[0m7.717    \u001b[0m | \u001b[0m100.8    \u001b[0m | \u001b[0m3.672    \u001b[0m | \u001b[0m3.092    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.8986   \u001b[0m | \u001b[0m0.2105   \u001b[0m | \u001b[0m0.8021   \u001b[0m | \u001b[0m0.06852  \u001b[0m | \u001b[0m5.158    \u001b[0m | \u001b[0m94.52    \u001b[0m | \u001b[0m4.861    \u001b[0m | \u001b[0m4.547    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.9336   \u001b[0m | \u001b[0m0.6765   \u001b[0m | \u001b[0m0.1152   \u001b[0m | \u001b[0m0.9584   \u001b[0m | \u001b[0m3.642    \u001b[0m | \u001b[0m124.1    \u001b[0m | \u001b[0m4.515    \u001b[0m | \u001b[0m1.699    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.9648   \u001b[0m | \u001b[0m0.6528   \u001b[0m | \u001b[0m0.9252   \u001b[0m | \u001b[0m0.3802   \u001b[0m | \u001b[0m8.4      \u001b[0m | \u001b[0m142.7    \u001b[0m | \u001b[0m3.101    \u001b[0m | \u001b[0m0.7553   \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.9535   \u001b[0m | \u001b[0m0.3749   \u001b[0m | \u001b[0m0.6358   \u001b[0m | \u001b[0m0.4645   \u001b[0m | \u001b[0m6.726    \u001b[0m | \u001b[0m104.7    \u001b[0m | \u001b[0m3.299    \u001b[0m | \u001b[0m2.383    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.9233   \u001b[0m | \u001b[0m0.4546   \u001b[0m | \u001b[0m0.1248   \u001b[0m | \u001b[0m0.4183   \u001b[0m | \u001b[0m4.912    \u001b[0m | \u001b[0m104.3    \u001b[0m | \u001b[0m4.872    \u001b[0m | \u001b[0m1.078    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.9028   \u001b[0m | \u001b[0m0.6828   \u001b[0m | \u001b[0m0.03181  \u001b[0m | \u001b[0m0.837    \u001b[0m | \u001b[0m7.912    \u001b[0m | \u001b[0m147.6    \u001b[0m | \u001b[0m2.959    \u001b[0m | \u001b[0m1.087    \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.9383   \u001b[0m | \u001b[0m0.3266   \u001b[0m | \u001b[0m0.1988   \u001b[0m | \u001b[0m0.4158   \u001b[0m | \u001b[0m8.034    \u001b[0m | \u001b[0m86.23    \u001b[0m | \u001b[0m1.007    \u001b[0m | \u001b[0m3.126    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.9549   \u001b[0m | \u001b[0m0.2138   \u001b[0m | \u001b[0m0.5877   \u001b[0m | \u001b[0m0.2882   \u001b[0m | \u001b[0m9.627    \u001b[0m | \u001b[0m96.77    \u001b[0m | \u001b[0m0.1686   \u001b[0m | \u001b[0m3.691    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.9135   \u001b[0m | \u001b[0m0.4669   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m9.154    \u001b[0m | \u001b[0m139.2    \u001b[0m | \u001b[0m3.479    \u001b[0m | \u001b[0m0.9228   \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.9631   \u001b[0m | \u001b[0m0.7972   \u001b[0m | \u001b[0m0.7192   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m6.1      \u001b[0m | \u001b[0m134.8    \u001b[0m | \u001b[0m1.532    \u001b[0m | \u001b[0m0.3317   \u001b[0m |\n",
      "=============================================================================================================\n",
      "It takes 57.16471598148346 minutes\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def gbm_cl_bo(colsample_bylevel, colsample_bytree, max_depth, learning_rate, n_estimators, reg_alpha, reg_lambda):\n",
    "    params_gbm = {}\n",
    "    params_gbm['colsample_bylevel'] = colsample_bylevel\n",
    "    params_gbm['colsample_bytree'] = colsample_bytree\n",
    "    params_gbm['max_depth'] = round(max_depth)\n",
    "    params_gbm['learning_rate'] = learning_rate\n",
    "    params_gbm['n_estimators'] = round(n_estimators)\n",
    "    params_gbm['reg_alpha'] = reg_alpha\n",
    "    params_gbm['reg_lambda'] = reg_lambda\n",
    "    scores = cross_val_score(XGBClassifier(random_state=123, **params_gbm),\n",
    "                             x_up_train, y_up_train, scoring='accuracy', cv=5).mean()\n",
    "    score = scores.mean()\n",
    "    return score\n",
    "# Run Bayesian Optimization\n",
    "start = time.time()\n",
    "params_gbm ={\n",
    "    'colsample_bylevel': (0,1),\n",
    "    'colsample_bytree': (0,1),\n",
    "    'max_depth':(3, 10),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'n_estimators':(80, 150),\n",
    "    'reg_alpha': (0,5),\n",
    "    'reg_lambda':(0,5) \n",
    "}\n",
    "gbm_bo = BayesianOptimization(gbm_cl_bo, params_gbm, random_state=111)\n",
    "gbm_bo.maximize(init_points=20, n_iter=4)\n",
    "print('It takes %s minutes' % ((time.time() - start)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
